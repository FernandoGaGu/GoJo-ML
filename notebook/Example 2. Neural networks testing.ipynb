{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1d6bf8-6a5e-4f16-9952-7e9a8bc2d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# DEID libraries\n",
    "from gojo import core\n",
    "from gojo import deepl\n",
    "from gojo import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db05344-f3fc-4116-bfd3-3795ae8eda1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13), '0.401', '0.389')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test dataset (Wine)\n",
    "wine_dt = datasets.load_wine()\n",
    "\n",
    "# create the target variable. Classification problem 0 vs rest\n",
    "# to see the target names you can use wine_dt['target_names']\n",
    "y = (wine_dt['target'] == 1).astype(int)  \n",
    "X = wine_dt['data']\n",
    "\n",
    "# standarize input data\n",
    "std_X = util.zscoresScaling(X)\n",
    "\n",
    "# split Xs and Ys in training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    std_X, y, train_size=0.8, random_state=1997, shuffle=True,\n",
    "    stratify=y\n",
    ")\n",
    "X_train.shape, X_valid.shape, '%.3f' % y_train.mean(),  '%.3f' % y_valid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ee04e1-b7d0-43eb-b6e4-dc05b422d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloaders\n",
    "train_dl = DataLoader(\n",
    "    deepl.loading.TorchDataset(X=X_train, y=y_train), \n",
    "    batch_size=16, shuffle=True)\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    deepl.loading.TorchDataset(X=X_valid, y=y_valid), \n",
    "    batch_size=X_valid.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45c6aed-e191-4273-92f0-a50629a78b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=20, bias=True)\n",
       "  (1): ELU(alpha=1.0)\n",
       "  (2): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a basic model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_train.shape[1], 20),\n",
    "    torch.nn.ELU(),\n",
    "    torch.nn.Linear(20, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cde6be-1b87-4540-91a3-56039ea120de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch (1) ============================================ \n",
      "\t (train) loss (mean): 0.69353\n",
      "\t (train) loss (std): 0.03923\n",
      "\t (train) accuracy: 0.50000\n",
      "\t (train) balanced_accuracy: 0.56213\n",
      "\t (train) precision: 0.43860\n",
      "\t (train) recall: 0.87719\n",
      "\t (train) sensitivity: 0.87719\n",
      "\t (train) specificity: 0.24706\n",
      "\t (train) negative_predictive_value: 0.75000\n",
      "\t (train) f1_score: 0.58480\n",
      "\t (train) auc: 0.56213\n",
      "\n",
      "\t (valid) loss (mean): 0.65316\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.58333\n",
      "\t (valid) balanced_accuracy: 0.64610\n",
      "\t (valid) precision: 0.48148\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.36364\n",
      "\t (valid) negative_predictive_value: 0.88889\n",
      "\t (valid) f1_score: 0.63415\n",
      "\t (valid) auc: 0.64610\n",
      "\n",
      "\n",
      "Epoch (2) ============================================ \n",
      "\t (train) loss (mean): 0.65382\n",
      "\t (train) loss (std): 0.03733\n",
      "\t (train) accuracy: 0.59155\n",
      "\t (train) balanced_accuracy: 0.64438\n",
      "\t (train) precision: 0.49524\n",
      "\t (train) recall: 0.91228\n",
      "\t (train) sensitivity: 0.91228\n",
      "\t (train) specificity: 0.37647\n",
      "\t (train) negative_predictive_value: 0.86486\n",
      "\t (train) f1_score: 0.64198\n",
      "\t (train) auc: 0.64438\n",
      "\n",
      "\t (valid) loss (mean): 0.61805\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.72222\n",
      "\t (valid) balanced_accuracy: 0.75974\n",
      "\t (valid) precision: 0.59091\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.59091\n",
      "\t (valid) negative_predictive_value: 0.92857\n",
      "\t (valid) f1_score: 0.72222\n",
      "\t (valid) auc: 0.75974\n",
      "\n",
      "\n",
      "Epoch (3) ============================================ \n",
      "\t (train) loss (mean): 0.61614\n",
      "\t (train) loss (std): 0.02864\n",
      "\t (train) accuracy: 0.67606\n",
      "\t (train) balanced_accuracy: 0.71496\n",
      "\t (train) precision: 0.55914\n",
      "\t (train) recall: 0.91228\n",
      "\t (train) sensitivity: 0.91228\n",
      "\t (train) specificity: 0.51765\n",
      "\t (train) negative_predictive_value: 0.89796\n",
      "\t (train) f1_score: 0.69333\n",
      "\t (train) auc: 0.71496\n",
      "\n",
      "\t (valid) loss (mean): 0.58190\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.77778\n",
      "\t (valid) balanced_accuracy: 0.80519\n",
      "\t (valid) precision: 0.65000\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.68182\n",
      "\t (valid) negative_predictive_value: 0.93750\n",
      "\t (valid) f1_score: 0.76471\n",
      "\t (valid) auc: 0.80519\n",
      "\n",
      "\n",
      "Epoch (4) ============================================ \n",
      "\t (train) loss (mean): 0.58065\n",
      "\t (train) loss (std): 0.05091\n",
      "\t (train) accuracy: 0.75352\n",
      "\t (train) balanced_accuracy: 0.78256\n",
      "\t (train) precision: 0.63095\n",
      "\t (train) recall: 0.92982\n",
      "\t (train) sensitivity: 0.92982\n",
      "\t (train) specificity: 0.63529\n",
      "\t (train) negative_predictive_value: 0.93103\n",
      "\t (train) f1_score: 0.75177\n",
      "\t (train) auc: 0.78256\n",
      "\n",
      "\t (valid) loss (mean): 0.55007\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.77778\n",
      "\t (valid) balanced_accuracy: 0.80519\n",
      "\t (valid) precision: 0.65000\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.68182\n",
      "\t (valid) negative_predictive_value: 0.93750\n",
      "\t (valid) f1_score: 0.76471\n",
      "\t (valid) auc: 0.80519\n",
      "\n",
      "\n",
      "Epoch (5) ============================================ \n",
      "\t (train) loss (mean): 0.54922\n",
      "\t (train) loss (std): 0.03972\n",
      "\t (train) accuracy: 0.81690\n",
      "\t (train) balanced_accuracy: 0.83839\n",
      "\t (train) precision: 0.70130\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.72941\n",
      "\t (train) negative_predictive_value: 0.95385\n",
      "\t (train) f1_score: 0.80597\n",
      "\t (train) auc: 0.83839\n",
      "\n",
      "\t (valid) loss (mean): 0.51695\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.83333\n",
      "\t (valid) balanced_accuracy: 0.85065\n",
      "\t (valid) precision: 0.72222\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.77273\n",
      "\t (valid) negative_predictive_value: 0.94444\n",
      "\t (valid) f1_score: 0.81250\n",
      "\t (valid) auc: 0.85065\n",
      "\n",
      "\n",
      "Epoch (6) ============================================ \n",
      "\t (train) loss (mean): 0.51658\n",
      "\t (train) loss (std): 0.04703\n",
      "\t (train) accuracy: 0.85915\n",
      "\t (train) balanced_accuracy: 0.87368\n",
      "\t (train) precision: 0.76056\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.80000\n",
      "\t (train) negative_predictive_value: 0.95775\n",
      "\t (train) f1_score: 0.84375\n",
      "\t (train) auc: 0.87368\n",
      "\n",
      "\t (valid) loss (mean): 0.48547\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.83333\n",
      "\t (valid) balanced_accuracy: 0.85065\n",
      "\t (valid) precision: 0.72222\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.77273\n",
      "\t (valid) negative_predictive_value: 0.94444\n",
      "\t (valid) f1_score: 0.81250\n",
      "\t (valid) auc: 0.85065\n",
      "\n",
      "\n",
      "Epoch (7) ============================================ \n",
      "\t (train) loss (mean): 0.48618\n",
      "\t (train) loss (std): 0.04557\n",
      "\t (train) accuracy: 0.89437\n",
      "\t (train) balanced_accuracy: 0.90310\n",
      "\t (train) precision: 0.81818\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.85882\n",
      "\t (train) negative_predictive_value: 0.96053\n",
      "\t (train) f1_score: 0.87805\n",
      "\t (train) auc: 0.90310\n",
      "\n",
      "\t (valid) loss (mean): 0.45542\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.86111\n",
      "\t (valid) balanced_accuracy: 0.87338\n",
      "\t (valid) precision: 0.76471\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.81818\n",
      "\t (valid) negative_predictive_value: 0.94737\n",
      "\t (valid) f1_score: 0.83871\n",
      "\t (valid) auc: 0.87338\n",
      "\n",
      "\n",
      "Epoch (8) ============================================ \n",
      "\t (train) loss (mean): 0.45870\n",
      "\t (train) loss (std): 0.03855\n",
      "\t (train) accuracy: 0.90141\n",
      "\t (train) balanced_accuracy: 0.90898\n",
      "\t (train) precision: 0.83077\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.87059\n",
      "\t (train) negative_predictive_value: 0.96104\n",
      "\t (train) f1_score: 0.88525\n",
      "\t (train) auc: 0.90898\n",
      "\n",
      "\t (valid) loss (mean): 0.42690\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.94444\n",
      "\t (valid) balanced_accuracy: 0.94156\n",
      "\t (valid) precision: 0.92857\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 0.95455\n",
      "\t (valid) f1_score: 0.92857\n",
      "\t (valid) auc: 0.94156\n",
      "\n",
      "\n",
      "Epoch (9) ============================================ \n",
      "\t (train) loss (mean): 0.43057\n",
      "\t (train) loss (std): 0.03824\n",
      "\t (train) accuracy: 0.90141\n",
      "\t (train) balanced_accuracy: 0.90898\n",
      "\t (train) precision: 0.83077\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.87059\n",
      "\t (train) negative_predictive_value: 0.96104\n",
      "\t (train) f1_score: 0.88525\n",
      "\t (train) auc: 0.90898\n",
      "\n",
      "\t (valid) loss (mean): 0.39994\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.94444\n",
      "\t (valid) balanced_accuracy: 0.94156\n",
      "\t (valid) precision: 0.92857\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 0.95455\n",
      "\t (valid) f1_score: 0.92857\n",
      "\t (valid) auc: 0.94156\n",
      "\n",
      "\n",
      "Epoch (10) ============================================ \n",
      "\t (train) loss (mean): 0.40345\n",
      "\t (train) loss (std): 0.04567\n",
      "\t (train) accuracy: 0.91549\n",
      "\t (train) balanced_accuracy: 0.92363\n",
      "\t (train) precision: 0.84615\n",
      "\t (train) recall: 0.96491\n",
      "\t (train) sensitivity: 0.96491\n",
      "\t (train) specificity: 0.88235\n",
      "\t (train) negative_predictive_value: 0.97403\n",
      "\t (train) f1_score: 0.90164\n",
      "\t (train) auc: 0.92363\n",
      "\n",
      "\t (valid) loss (mean): 0.37372\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.94444\n",
      "\t (valid) balanced_accuracy: 0.94156\n",
      "\t (valid) precision: 0.92857\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 0.95455\n",
      "\t (valid) f1_score: 0.92857\n",
      "\t (valid) auc: 0.94156\n",
      "\n",
      "\n",
      "Epoch (11) ============================================ \n",
      "\t (train) loss (mean): 0.37857\n",
      "\t (train) loss (std): 0.04999\n",
      "\t (train) accuracy: 0.92958\n",
      "\t (train) balanced_accuracy: 0.93540\n",
      "\t (train) precision: 0.87302\n",
      "\t (train) recall: 0.96491\n",
      "\t (train) sensitivity: 0.96491\n",
      "\t (train) specificity: 0.90588\n",
      "\t (train) negative_predictive_value: 0.97468\n",
      "\t (train) f1_score: 0.91667\n",
      "\t (train) auc: 0.93540\n",
      "\n",
      "\t (valid) loss (mean): 0.35020\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.94444\n",
      "\t (valid) balanced_accuracy: 0.94156\n",
      "\t (valid) precision: 0.92857\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 0.95455\n",
      "\t (valid) f1_score: 0.92857\n",
      "\t (valid) auc: 0.94156\n",
      "\n",
      "\n",
      "Epoch (12) ============================================ \n",
      "\t (train) loss (mean): 0.35393\n",
      "\t (train) loss (std): 0.04934\n",
      "\t (train) accuracy: 0.94366\n",
      "\t (train) balanced_accuracy: 0.94716\n",
      "\t (train) precision: 0.90164\n",
      "\t (train) recall: 0.96491\n",
      "\t (train) sensitivity: 0.96491\n",
      "\t (train) specificity: 0.92941\n",
      "\t (train) negative_predictive_value: 0.97531\n",
      "\t (train) f1_score: 0.93220\n",
      "\t (train) auc: 0.94716\n",
      "\n",
      "\t (valid) loss (mean): 0.32645\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.94444\n",
      "\t (valid) balanced_accuracy: 0.94156\n",
      "\t (valid) precision: 0.92857\n",
      "\t (valid) recall: 0.92857\n",
      "\t (valid) sensitivity: 0.92857\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 0.95455\n",
      "\t (valid) f1_score: 0.92857\n",
      "\t (valid) auc: 0.94156\n",
      "\n",
      "\n",
      "Epoch (13) ============================================ \n",
      "\t (train) loss (mean): 0.33337\n",
      "\t (train) loss (std): 0.05641\n",
      "\t (train) accuracy: 0.94366\n",
      "\t (train) balanced_accuracy: 0.94716\n",
      "\t (train) precision: 0.90164\n",
      "\t (train) recall: 0.96491\n",
      "\t (train) sensitivity: 0.96491\n",
      "\t (train) specificity: 0.92941\n",
      "\t (train) negative_predictive_value: 0.97531\n",
      "\t (train) f1_score: 0.93220\n",
      "\t (train) auc: 0.94716\n",
      "\n",
      "\t (valid) loss (mean): 0.30431\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (14) ============================================ \n",
      "\t (train) loss (mean): 0.31108\n",
      "\t (train) loss (std): 0.04329\n",
      "\t (train) accuracy: 0.95070\n",
      "\t (train) balanced_accuracy: 0.95015\n",
      "\t (train) precision: 0.93103\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.95294\n",
      "\t (train) negative_predictive_value: 0.96429\n",
      "\t (train) f1_score: 0.93913\n",
      "\t (train) auc: 0.95015\n",
      "\n",
      "\t (valid) loss (mean): 0.28398\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (15) ============================================ \n",
      "\t (train) loss (mean): 0.29125\n",
      "\t (train) loss (std): 0.02759\n",
      "\t (train) accuracy: 0.95070\n",
      "\t (train) balanced_accuracy: 0.95015\n",
      "\t (train) precision: 0.93103\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.95294\n",
      "\t (train) negative_predictive_value: 0.96429\n",
      "\t (train) f1_score: 0.93913\n",
      "\t (train) auc: 0.95015\n",
      "\n",
      "\t (valid) loss (mean): 0.26535\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (16) ============================================ \n",
      "\t (train) loss (mean): 0.27437\n",
      "\t (train) loss (std): 0.03473\n",
      "\t (train) accuracy: 0.95070\n",
      "\t (train) balanced_accuracy: 0.95015\n",
      "\t (train) precision: 0.93103\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.95294\n",
      "\t (train) negative_predictive_value: 0.96429\n",
      "\t (train) f1_score: 0.93913\n",
      "\t (train) auc: 0.95015\n",
      "\n",
      "\t (valid) loss (mean): 0.24844\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (17) ============================================ \n",
      "\t (train) loss (mean): 0.25610\n",
      "\t (train) loss (std): 0.04605\n",
      "\t (train) accuracy: 0.95070\n",
      "\t (train) balanced_accuracy: 0.95015\n",
      "\t (train) precision: 0.93103\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.95294\n",
      "\t (train) negative_predictive_value: 0.96429\n",
      "\t (train) f1_score: 0.93913\n",
      "\t (train) auc: 0.95015\n",
      "\n",
      "\t (valid) loss (mean): 0.23241\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (18) ============================================ \n",
      "\t (train) loss (mean): 0.24084\n",
      "\t (train) loss (std): 0.04763\n",
      "\t (train) accuracy: 0.95775\n",
      "\t (train) balanced_accuracy: 0.95604\n",
      "\t (train) precision: 0.94737\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.96471\n",
      "\t (train) f1_score: 0.94737\n",
      "\t (train) auc: 0.95604\n",
      "\n",
      "\t (valid) loss (mean): 0.21792\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (19) ============================================ \n",
      "\t (train) loss (mean): 0.22623\n",
      "\t (train) loss (std): 0.05795\n",
      "\t (train) accuracy: 0.95775\n",
      "\t (train) balanced_accuracy: 0.95604\n",
      "\t (train) precision: 0.94737\n",
      "\t (train) recall: 0.94737\n",
      "\t (train) sensitivity: 0.94737\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.96471\n",
      "\t (train) f1_score: 0.94737\n",
      "\t (train) auc: 0.95604\n",
      "\n",
      "\t (valid) loss (mean): 0.20422\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (20) ============================================ \n",
      "\t (train) loss (mean): 0.21356\n",
      "\t (train) loss (std): 0.05633\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.19250\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (21) ============================================ \n",
      "\t (train) loss (mean): 0.20212\n",
      "\t (train) loss (std): 0.04893\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.18225\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (22) ============================================ \n",
      "\t (train) loss (mean): 0.18950\n",
      "\t (train) loss (std): 0.04384\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.17236\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (23) ============================================ \n",
      "\t (train) loss (mean): 0.17971\n",
      "\t (train) loss (std): 0.04930\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.16276\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (24) ============================================ \n",
      "\t (train) loss (mean): 0.16947\n",
      "\t (train) loss (std): 0.05029\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.15441\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (25) ============================================ \n",
      "\t (train) loss (mean): 0.16114\n",
      "\t (train) loss (std): 0.04079\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.14728\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (26) ============================================ \n",
      "\t (train) loss (mean): 0.15376\n",
      "\t (train) loss (std): 0.04231\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.14002\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (27) ============================================ \n",
      "\t (train) loss (mean): 0.14551\n",
      "\t (train) loss (std): 0.04101\n",
      "\t (train) accuracy: 0.97183\n",
      "\t (train) balanced_accuracy: 0.97358\n",
      "\t (train) precision: 0.94915\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.96471\n",
      "\t (train) negative_predictive_value: 0.98795\n",
      "\t (train) f1_score: 0.96552\n",
      "\t (train) auc: 0.97358\n",
      "\n",
      "\t (valid) loss (mean): 0.13282\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (28) ============================================ \n",
      "\t (train) loss (mean): 0.13917\n",
      "\t (train) loss (std): 0.04335\n",
      "\t (train) accuracy: 0.97887\n",
      "\t (train) balanced_accuracy: 0.97946\n",
      "\t (train) precision: 0.96552\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.97647\n",
      "\t (train) negative_predictive_value: 0.98810\n",
      "\t (train) f1_score: 0.97391\n",
      "\t (train) auc: 0.97946\n",
      "\n",
      "\t (valid) loss (mean): 0.12726\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (29) ============================================ \n",
      "\t (train) loss (mean): 0.13231\n",
      "\t (train) loss (std): 0.04604\n",
      "\t (train) accuracy: 0.97887\n",
      "\t (train) balanced_accuracy: 0.97946\n",
      "\t (train) precision: 0.96552\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.97647\n",
      "\t (train) negative_predictive_value: 0.98810\n",
      "\t (train) f1_score: 0.97391\n",
      "\t (train) auc: 0.97946\n",
      "\n",
      "\t (valid) loss (mean): 0.12173\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (30) ============================================ \n",
      "\t (train) loss (mean): 0.12679\n",
      "\t (train) loss (std): 0.03523\n",
      "\t (train) accuracy: 0.97887\n",
      "\t (train) balanced_accuracy: 0.97946\n",
      "\t (train) precision: 0.96552\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.97647\n",
      "\t (train) negative_predictive_value: 0.98810\n",
      "\t (train) f1_score: 0.97391\n",
      "\t (train) auc: 0.97946\n",
      "\n",
      "\t (valid) loss (mean): 0.11677\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (31) ============================================ \n",
      "\t (train) loss (mean): 0.12093\n",
      "\t (train) loss (std): 0.04694\n",
      "\t (train) accuracy: 0.97887\n",
      "\t (train) balanced_accuracy: 0.97946\n",
      "\t (train) precision: 0.96552\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.97647\n",
      "\t (train) negative_predictive_value: 0.98810\n",
      "\t (train) f1_score: 0.97391\n",
      "\t (train) auc: 0.97946\n",
      "\n",
      "\t (valid) loss (mean): 0.11189\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (32) ============================================ \n",
      "\t (train) loss (mean): 0.11559\n",
      "\t (train) loss (std): 0.04107\n",
      "\t (train) accuracy: 0.98592\n",
      "\t (train) balanced_accuracy: 0.98535\n",
      "\t (train) precision: 0.98246\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.98824\n",
      "\t (train) negative_predictive_value: 0.98824\n",
      "\t (train) f1_score: 0.98246\n",
      "\t (train) auc: 0.98535\n",
      "\n",
      "\t (valid) loss (mean): 0.10752\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (33) ============================================ \n",
      "\t (train) loss (mean): 0.11174\n",
      "\t (train) loss (std): 0.03036\n",
      "\t (train) accuracy: 0.98592\n",
      "\t (train) balanced_accuracy: 0.98535\n",
      "\t (train) precision: 0.98246\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 0.98824\n",
      "\t (train) negative_predictive_value: 0.98824\n",
      "\t (train) f1_score: 0.98246\n",
      "\t (train) auc: 0.98535\n",
      "\n",
      "\t (valid) loss (mean): 0.10340\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (34) ============================================ \n",
      "\t (train) loss (mean): 0.10791\n",
      "\t (train) loss (std): 0.02840\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.09969\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (35) ============================================ \n",
      "\t (train) loss (mean): 0.10416\n",
      "\t (train) loss (std): 0.04140\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.09632\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (36) ============================================ \n",
      "\t (train) loss (mean): 0.09951\n",
      "\t (train) loss (std): 0.02611\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.09298\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (37) ============================================ \n",
      "\t (train) loss (mean): 0.09620\n",
      "\t (train) loss (std): 0.01989\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.08975\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (38) ============================================ \n",
      "\t (train) loss (mean): 0.09323\n",
      "\t (train) loss (std): 0.02368\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.08656\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (39) ============================================ \n",
      "\t (train) loss (mean): 0.08957\n",
      "\t (train) loss (std): 0.03934\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.08376\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (40) ============================================ \n",
      "\t (train) loss (mean): 0.08831\n",
      "\t (train) loss (std): 0.04312\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.08129\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (41) ============================================ \n",
      "\t (train) loss (mean): 0.08366\n",
      "\t (train) loss (std): 0.03503\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.07882\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (42) ============================================ \n",
      "\t (train) loss (mean): 0.08216\n",
      "\t (train) loss (std): 0.04491\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.07660\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (43) ============================================ \n",
      "\t (train) loss (mean): 0.07893\n",
      "\t (train) loss (std): 0.03436\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.07427\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (44) ============================================ \n",
      "\t (train) loss (mean): 0.07860\n",
      "\t (train) loss (std): 0.04601\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.07231\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (45) ============================================ \n",
      "\t (train) loss (mean): 0.07478\n",
      "\t (train) loss (std): 0.02881\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.07023\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (46) ============================================ \n",
      "\t (train) loss (mean): 0.07413\n",
      "\t (train) loss (std): 0.03439\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.06841\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (47) ============================================ \n",
      "\t (train) loss (mean): 0.07096\n",
      "\t (train) loss (std): 0.03356\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.06648\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (48) ============================================ \n",
      "\t (train) loss (mean): 0.06910\n",
      "\t (train) loss (std): 0.03407\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.06475\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (49) ============================================ \n",
      "\t (train) loss (mean): 0.06758\n",
      "\t (train) loss (std): 0.02439\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.06320\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n",
      "\n",
      "Epoch (50) ============================================ \n",
      "\t (train) loss (mean): 0.06548\n",
      "\t (train) loss (std): 0.04272\n",
      "\t (train) accuracy: 0.99296\n",
      "\t (train) balanced_accuracy: 0.99123\n",
      "\t (train) precision: 1.00000\n",
      "\t (train) recall: 0.98246\n",
      "\t (train) sensitivity: 0.98246\n",
      "\t (train) specificity: 1.00000\n",
      "\t (train) negative_predictive_value: 0.98837\n",
      "\t (train) f1_score: 0.99115\n",
      "\t (train) auc: 0.99123\n",
      "\n",
      "\t (valid) loss (mean): 0.06157\n",
      "\t (valid) loss (std): 0.00000\n",
      "\t (valid) accuracy: 0.97222\n",
      "\t (valid) balanced_accuracy: 0.97727\n",
      "\t (valid) precision: 0.93333\n",
      "\t (valid) recall: 1.00000\n",
      "\t (valid) sensitivity: 1.00000\n",
      "\t (valid) specificity: 0.95455\n",
      "\t (valid) negative_predictive_value: 1.00000\n",
      "\t (valid) f1_score: 0.96552\n",
      "\t (valid) auc: 0.97727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = deepl.fitNeuralNetwork(\n",
    "    deepl.iterSupervisedEpoch,\n",
    "    model=model,\n",
    "    train_dl=train_dl,\n",
    "    valid_dl=valid_dl,\n",
    "    n_epochs=50,\n",
    "    loss_fn=torch.nn.BCELoss(),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    optimizer_params={'lr': 0.001},\n",
    "    device='mps',\n",
    "    metrics=core.getDefaultMetrics('binary_classification', bin_threshold=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f39580-cea5-41d0-990b-b089b9d3fe35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>negative_predictive_value</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.562126</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.584795</td>\n",
       "      <td>0.562126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.644376</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>0.644376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.714964</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.714964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753521</td>\n",
       "      <td>0.782559</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.751773</td>\n",
       "      <td>0.782559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.838390</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.838390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.873684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.903096</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.903096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.908978</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.908978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.908978</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.908978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.923633</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.923633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.935397</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.935397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.947162</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.947162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.943662</td>\n",
       "      <td>0.947162</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.947162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.950155</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.950155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.950155</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.950155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.950155</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.950155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.950155</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.950155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.956037</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.956037</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.979463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.979463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.979463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.979463</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.979463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985346</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.985346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.985346</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.985346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  balanced_accuracy  precision    recall  sensitivity  \\\n",
       "0   0.500000           0.562126   0.438596  0.877193     0.877193   \n",
       "1   0.591549           0.644376   0.495238  0.912281     0.912281   \n",
       "2   0.676056           0.714964   0.559140  0.912281     0.912281   \n",
       "3   0.753521           0.782559   0.630952  0.929825     0.929825   \n",
       "4   0.816901           0.838390   0.701299  0.947368     0.947368   \n",
       "5   0.859155           0.873684   0.760563  0.947368     0.947368   \n",
       "6   0.894366           0.903096   0.818182  0.947368     0.947368   \n",
       "7   0.901408           0.908978   0.830769  0.947368     0.947368   \n",
       "8   0.901408           0.908978   0.830769  0.947368     0.947368   \n",
       "9   0.915493           0.923633   0.846154  0.964912     0.964912   \n",
       "10  0.929577           0.935397   0.873016  0.964912     0.964912   \n",
       "11  0.943662           0.947162   0.901639  0.964912     0.964912   \n",
       "12  0.943662           0.947162   0.901639  0.964912     0.964912   \n",
       "13  0.950704           0.950155   0.931034  0.947368     0.947368   \n",
       "14  0.950704           0.950155   0.931034  0.947368     0.947368   \n",
       "15  0.950704           0.950155   0.931034  0.947368     0.947368   \n",
       "16  0.950704           0.950155   0.931034  0.947368     0.947368   \n",
       "17  0.957746           0.956037   0.947368  0.947368     0.947368   \n",
       "18  0.957746           0.956037   0.947368  0.947368     0.947368   \n",
       "19  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "20  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "21  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "22  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "23  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "24  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "25  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "26  0.971831           0.973581   0.949153  0.982456     0.982456   \n",
       "27  0.978873           0.979463   0.965517  0.982456     0.982456   \n",
       "28  0.978873           0.979463   0.965517  0.982456     0.982456   \n",
       "29  0.978873           0.979463   0.965517  0.982456     0.982456   \n",
       "30  0.978873           0.979463   0.965517  0.982456     0.982456   \n",
       "31  0.985915           0.985346   0.982456  0.982456     0.982456   \n",
       "32  0.985915           0.985346   0.982456  0.982456     0.982456   \n",
       "33  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "34  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "35  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "36  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "37  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "38  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "39  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "40  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "41  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "42  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "43  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "44  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "45  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "46  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "47  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "48  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "49  0.992958           0.991228   1.000000  0.982456     0.982456   \n",
       "\n",
       "    specificity  negative_predictive_value  f1_score       auc  \n",
       "0      0.247059                   0.750000  0.584795  0.562126  \n",
       "1      0.376471                   0.864865  0.641975  0.644376  \n",
       "2      0.517647                   0.897959  0.693333  0.714964  \n",
       "3      0.635294                   0.931034  0.751773  0.782559  \n",
       "4      0.729412                   0.953846  0.805970  0.838390  \n",
       "5      0.800000                   0.957746  0.843750  0.873684  \n",
       "6      0.858824                   0.960526  0.878049  0.903096  \n",
       "7      0.870588                   0.961039  0.885246  0.908978  \n",
       "8      0.870588                   0.961039  0.885246  0.908978  \n",
       "9      0.882353                   0.974026  0.901639  0.923633  \n",
       "10     0.905882                   0.974684  0.916667  0.935397  \n",
       "11     0.929412                   0.975309  0.932203  0.947162  \n",
       "12     0.929412                   0.975309  0.932203  0.947162  \n",
       "13     0.952941                   0.964286  0.939130  0.950155  \n",
       "14     0.952941                   0.964286  0.939130  0.950155  \n",
       "15     0.952941                   0.964286  0.939130  0.950155  \n",
       "16     0.952941                   0.964286  0.939130  0.950155  \n",
       "17     0.964706                   0.964706  0.947368  0.956037  \n",
       "18     0.964706                   0.964706  0.947368  0.956037  \n",
       "19     0.964706                   0.987952  0.965517  0.973581  \n",
       "20     0.964706                   0.987952  0.965517  0.973581  \n",
       "21     0.964706                   0.987952  0.965517  0.973581  \n",
       "22     0.964706                   0.987952  0.965517  0.973581  \n",
       "23     0.964706                   0.987952  0.965517  0.973581  \n",
       "24     0.964706                   0.987952  0.965517  0.973581  \n",
       "25     0.964706                   0.987952  0.965517  0.973581  \n",
       "26     0.964706                   0.987952  0.965517  0.973581  \n",
       "27     0.976471                   0.988095  0.973913  0.979463  \n",
       "28     0.976471                   0.988095  0.973913  0.979463  \n",
       "29     0.976471                   0.988095  0.973913  0.979463  \n",
       "30     0.976471                   0.988095  0.973913  0.979463  \n",
       "31     0.988235                   0.988235  0.982456  0.985346  \n",
       "32     0.988235                   0.988235  0.982456  0.985346  \n",
       "33     1.000000                   0.988372  0.991150  0.991228  \n",
       "34     1.000000                   0.988372  0.991150  0.991228  \n",
       "35     1.000000                   0.988372  0.991150  0.991228  \n",
       "36     1.000000                   0.988372  0.991150  0.991228  \n",
       "37     1.000000                   0.988372  0.991150  0.991228  \n",
       "38     1.000000                   0.988372  0.991150  0.991228  \n",
       "39     1.000000                   0.988372  0.991150  0.991228  \n",
       "40     1.000000                   0.988372  0.991150  0.991228  \n",
       "41     1.000000                   0.988372  0.991150  0.991228  \n",
       "42     1.000000                   0.988372  0.991150  0.991228  \n",
       "43     1.000000                   0.988372  0.991150  0.991228  \n",
       "44     1.000000                   0.988372  0.991150  0.991228  \n",
       "45     1.000000                   0.988372  0.991150  0.991228  \n",
       "46     1.000000                   0.988372  0.991150  0.991228  \n",
       "47     1.000000                   0.988372  0.991150  0.991228  \n",
       "48     1.000000                   0.988372  0.991150  0.991228  \n",
       "49     1.000000                   0.988372  0.991150  0.991228  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(output['train_metrics']).join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b17d426-58ab-425a-909f-ec3e22bb1b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGHElEQVR4nO3de1zT970/8FcSIEEuAUTDVUTrBUQpgiJQ7R2ltUdP18nWla6ttvO0qzK6njNmb7ruULvNU6+srjrrOf6UttbWnaOtdF1VhlWh4AVvWLEgBikoCRe5Jd/fHyHRlFsSciN5PR+PPBK/+Xy/eec7Z179fD+fz1ckCIIAIiIiIicmdnQBRERERINhYCEiIiKnx8BCRERETo+BhYiIiJweAwsRERE5PQYWIiIicnoMLEREROT0GFiIiIjI6Xk4ugBr0Wq1uHr1Kvz8/CASiRxdDhEREZlAEAQ0NzcjLCwMYnH//SguE1iuXr2KyMhIR5dBREREFqipqUFERES/77tMYPHz8wOg+8L+/v4OroaIiIhMoVarERkZafgd74/LBBb9ZSB/f38GFiIiomFmsOEcHHRLRERETo+BhYiIiJye2YHl0KFDeOSRRxAWFgaRSIRPPvlk0H0OHjyIxMREyGQyjBs3Dn/+8597tdm9ezdiY2MhlUoRGxuLPXv2mFsaERERuSizA0trayvi4+OxYcMGk9pXVVXhoYcewuzZs1FWVobf/va3WLZsGXbv3m1oc+TIEWRmZiIrKwsnTpxAVlYWFi1ahKNHj5pbHhEREbkgkSAIgsU7i0TYs2cPFi5c2G+b//iP/8DevXtx9uxZw7alS5fixIkTOHLkCAAgMzMTarUa+/fvN7SZN28eAgMDsXPnTpNqUavVkMvlUKlUHHRLREQ0TJj6+23zMSxHjhxBenq60ba5c+eipKQEXV1dA7YpLi7u97gdHR1Qq9VGDyIiInJNNg8sdXV1UCgURtsUCgW6u7vR0NAwYJu6urp+j5uXlwe5XG54cNE4IiIi12WXWUI/nFutvwp1+/a+2gw0Jzs3NxcqlcrwqKmpsWLFRERE5ExsvnBcSEhIr56S+vp6eHh4YOTIkQO2+WGvy+2kUimkUqn1CyYiIiKnY/MelpSUFBQWFhptO3DgAJKSkuDp6Tlgm9TUVFuXR0RERMOA2T0sLS0tuHjxouHPVVVVKC8vR1BQEMaMGYPc3FzU1tZi+/btAHQzgjZs2ICcnBw8++yzOHLkCLZs2WI0+2f58uWYM2cOVq9ejQULFuDTTz/FF198gaKiIit8RSIiIhruzO5hKSkpQUJCAhISEgAAOTk5SEhIwGuvvQYAUCqVqK6uNrSPjo7Gvn378NVXX+HOO+/E7373O6xbtw4/+tGPDG1SU1Oxa9cu/PWvf8W0adOwbds2FBQUIDk5eajfj4iIiFzAkNZhcSZch4WI6BaNVsAHJTW4cK3Z0aWQC3kmLRqRQSOsekxTf79d5m7NRESk09TWieW7ynHwwveOLoVczCPxYVYPLKZiYCEiciGna1VY+j+luHLjJmSeYvwsOQoyT97nlqxD4S9z2GczsBARuYgPSmrwyien0dmtxZigEfjzE4mIDeMlcnINDCxERMNcR7cGb+w9g53HdBMe7p88GmsW3Qn5CE8HV0ZkPQwsRETDWG3TTTz/P6U4cUUFkQjIeWAiXrj3DojF/a8UTjQcMbAQEQ1TRZUNeHHnN7jR1oWAEZ5Y+5ME3D1xlKPLIrIJBhYiIifS3qXBl+fq0drRPWC7Sw2tePfgt9AKQFy4P/J/luiw2RtE9sDAQkTkJKoaWvFv/1OKc3Wmr52yKCkCqxbEQeYpsWFlRI7HwEJE5AT2n1Li5Y9OoqWjGyN9vDAtQj5ge4lYhIemhuLR6RF2qpDIsRhYiIgcqEujxVv7z2FLURUAYObYIKx/PMGh610QOSMGFiIiB6lTteOF//cNSr+7AQD4xZxxeHnuJHhIuNAb0Q8xsBAROcA/LzZg2c4yNLZ2wk/qgT8uisfcKSGOLovIaTGwEBHZkVYrYOM/LmLNFxcgCEBMqD/yfzYdY4N9HF0akVNjYCEityUIAr6pbkJDS4edPg8oOF6Nf5zX3ZSQM3yITMfAQkRuqaWjG7kfn8LfTly1+2dLPcT43YI4LJoRaffPJhquGFiIyO2cVarxwo5vcKmhFRKxCNMi5LDXQvaBI7yQkz4RU8IGnrZMRMYYWIjIbQiCgILjNXh9bwU6urUIlcuw4fEEJEYFObo0IhoEAwsRuYW2zm68suc0Pi6rBQDcM2kU1iy6E0E+Xg6ujIhMwcBCRC7vwrVmPL/jG1ysb4FELMJL6ROxdM543tGYaBhhYCEil/ZR6RW8+slp3OzSYLSfFOt/moDkcSMdXRYRmYmBhVyTIACVhUBbo6MrcZjvWzpwuaHV0WU4VGV9C0ouX0cGgIlhfvhZ8hj4qb8Hyh1dGdEwNeFBwCfYIR/NwEKu6dz/AgVPOLoKhxrV83BnMwA8rh+ich3AfgcWQ+QKFn/BwEJkVRWf6J6DJwIBYxxaiiOcq2tGnaodUg8xfKTu+39zsQgIDxyBwBGeji6FyDXI/B320e77Lxm5Lk0XcLFQ9/pf1gNjZjm2Hjs7VnUdi949ApEI2P1sKqaNCXR0SUREQ8ZbgpLrqf4aaFcBI0YCETMcXY1ddWm0eOWTUwCAn8wYg+kMK0TkIhhYyPVc+Ez3PCEdELvXPVq2FFXhwrUWjPTxwn/Mm+TocoiIrIaBhVzP+Z6RlRPnObYOO7tyow1rv6gEAOQ+FIOAEVwQjYhcBwMLuZaGSuD6t4DYExh/n6OrsauVfzuDm10azIwOwo+mhzu6HCIiq2JgIddyfp/ueexdDh3Nbm9fnLmGwjPX4CEW4c2FcRCJuIIrEbkWBhZyLed7xq9MynBsHXbU1tmN1/dWAACWzB6HiQo/B1dERGR9FgWWTZs2ITo6GjKZDImJiTh8+PCA7Tdu3IiYmBh4e3tj0qRJ2L59u9H727Ztg0gk6vVob2+3pDxyV23XgZqvda/daPzK+i8vorbpJsIDvLHs/jscXQ4RkU2YvQ5LQUEBsrOzsWnTJqSlpeHdd99FRkYGzpw5gzFjei/QlZ+fj9zcXPzlL3/BjBkzcOzYMTz77LMIDAzEI488Ymjn7++P8+fPG+0rk8ks+ErktioLAUELjJ4CBEY5uhq7qLzWjL8cugQAeONfpmCEF5dWIiLXZPa/bmvWrMHixYuxZMkSAMA777yDzz//HPn5+cjLy+vV/r//+7/xi1/8ApmZmQCAcePG4euvv8bq1auNAotIJEJISIil34MIuNAzO2iSe/SuCIKAVz45jW6tgAdiFHgwVuHokoiIbMasS0KdnZ0oLS1Fenq60fb09HQUFxf3uU9HR0evnhJvb28cO3YMXV1dhm0tLS2IiopCREQE5s+fj7KyMnNKI3fX3Qlc/Lvu9UT3GL/y8Te1OFp1Hd6eErzxL7GOLoeIyKbMCiwNDQ3QaDRQKIz/S06hUKCurq7PfebOnYv33nsPpaWlEAQBJSUl2Lp1K7q6utDQ0AAAmDx5MrZt24a9e/di586dkMlkSEtLQ2VlZb+1dHR0QK1WGz3IjVUXAx1qwGcUEJ7o6GpsrqmtE7/fdxYAsOz+CYgIHOHgioiIbMuiC94/nDIpCEK/0yhfffVV1NXVYdasWRAEAQqFAk899RTefvttSCS6VUhnzZqFWbNu3e8lLS0N06dPx/r167Fu3bo+j5uXl4eVK1daUj65Iv3soAlzAfHwnfym1Qo4VPk9rrd2Dtiu8Mw1XG/txITRvlh8V7SdqiMichyzAktwcDAkEkmv3pT6+vpevS563t7e2Lp1K959911cu3YNoaGh2Lx5M/z8/BAc3PctqsViMWbMmDFgD0tubi5ycnIMf1ar1YiMjDTn65CrEASXGL/S0NKBXxWU43Blg8n7vLkwDl4ewzegERGZyqzA4uXlhcTERBQWFuJf//VfDdsLCwuxYMGCAff19PREREQEAGDXrl2YP38+xP38l7AgCCgvL8fUqVP7PZ5UKoVUKjWnfHJV358HblwGJF7AuHsdXY1Fii82YHlBOb5v7oDMU4wZY4MG3eeeSaORPG6kHaojInI8sy8J5eTkICsrC0lJSUhJScHmzZtRXV2NpUuXAtD1fNTW1hrWWrlw4QKOHTuG5ORk3LhxA2vWrMHp06fx/vvvG465cuVKzJo1CxMmTIBarca6detQXl6OjRs3WulrkkvT965EzwGkvo6txUzdGi3W/b0S6/9xEYIATBjti40/m87F34iIfsDswJKZmYnGxkasWrUKSqUScXFx2LdvH6KidOteKJVKVFdXG9prNBr86U9/wvnz5+Hp6Yl7770XxcXFGDt2rKFNU1MTnnvuOdTV1UEulyMhIQGHDh3CzJkzh/4NyfXpb3Y4zFa3rVO1Y9muMhyrug4A+MmMSLz+yBR4e7nXHaaJiEwhEgRBcHQR1qBWqyGXy6FSqeDv7z73kHF7rQ3AH+4AIAC/qgDkEY6uyCRfnruGlz44gRttXfDxkuA/H52KBXfyhoVE5H5M/f3mspg0vFUeACAAIVOHRVjp7NbiD5+fw18OVwEA4sL9seGn0zE22MfBlREROTcGFhre9JeDLFgsrqNbg4LjNahubLNyUf07dvk6Tl5RAQCeSh2L3IcmQ+rBS0BERINhYKHhq7sD+PZL3WszpzMf+bYRKz45hUvft9qgsIHJvT3x9mPTMHcKb0VBRGQqBhYavi4XAZ0tgK8CCE0waZfrrZ34/f+dxe5vrgAAgn2l+NeEMEjstNict6cEjyVFIDzA2y6fR0TkKhhYaPi60LO67cTBV7cVBAEfllzBf+4/i6a2LohEwM+Sx+DluZMh9/a0Q7FERDQUDCw0PAnCreX4Jz00YNOL9c347Z7ThunDk0P8kPfoVCSMCbR1lUREZCUMLDQ81Z8BVNWAhwyIvrvPJu1dGmz48iLePfQtujQCvD0l+NWDE/B0WjQ8JVzOnohoOGFgoWHn60uNuPDhn/EkgK8xFa9vLOmzXWNrJxpaOgAA908ejZULpvCuxkREwxQDCw0r3zd34Jf/7xts7iwGxMCnN6fhfEtzv+1D/GV4419iMXdKSL93FCciIufHwELDhiAI+PePTgAt3+NO2bcAgEczF2P+iL7vFC4WiRAfKccIL/41JyIa7vgvOQ2spR7YvUS3BL6D3WjrxL+r2+Er7YAYAhB6J2ZMm+LosoiIyA4YWGhg5/4XqDro6CoAAEEAgm4fKzv1MUeVQkREdsbAQgNr6rnz9uT5wIwlDimhU6PFq5+expUbN5EQGYCX0idB5OUDhCc6pB4iIrI/BhYamD6wRCYD4+91SAm///Q0ChrbEewrxdqs2RD5Sh1SBxEROQ4Xo6CB6QNLwBiHfPyX567h/SPfAQD++ONpCGZYISJySwwsNLCmGt2zAwJLfXM7fv3hSQDAM2nRuGfSaLvXQEREzoGBhfrX1Q601OleB0TZ9aO1WgG//vAkrrd2YnKIH/593iS7fj4RETkXBhbqn0p3R2N4jgBGBNn1o/9afBmHLnwPqYcY63+aAJmnxK6fT0REzoWBhfqnum38ih1Xia24qsLq/ecAAK/Mj8UEhZ/dPpuIiJwTAwv1zwEDbm92arB8Vzk6NVo8EKPAE8mOGexLRETOhYGF+qcPLPJIu33km/93BhfrWzDaT4q3H5vG+/8QEREABhYaiJ17WP7vpBI7juo+80+L4hHk42WXzyUiIufHwEL9s+OU5urGNvxmt24K8/P3jMfsCaNs/plERDR8MLBQ/ww9LLad0tzZrcUvd36D5o5uJEYFIufBiTb9PCIiGn4YWKhv3Z1As1L3OsC2Y1je/uwcTl5RQe7tiXU/TYCHhH8tiYjIGH8ZqG/qKwAEwEMG+Nju8szfz17De0VVAIA//jge4QHeNvssIiIavhhYqG9Ntl+DRam6iZc+PAEAeDptLB6MVdjkc4iIaPhjYKG+2XhKc7dGi2U7y9DU1oWp4XL8JmOyTT6HiIhcAwML9c3GM4Te+aISxy/fgK/UAxseT4DUg0vvExFR/xhYqG82XIOlqLIBG7+6CADIe3Qqokb6WP0ziIjItTCwUN9sFFjqm9uRXVAOQQB+OnMMHokPs+rxiYjINVkUWDZt2oTo6GjIZDIkJibi8OHDA7bfuHEjYmJi4O3tjUmTJmH79u292uzevRuxsbGQSqWIjY3Fnj17LCmNrMUGgUWrFZBTcAINLR2YpPDD64/EWu3YRETk2swOLAUFBcjOzsaKFStQVlaG2bNnIyMjA9XV1X22z8/PR25uLt544w1UVFRg5cqVeOGFF/C3v/3N0ObIkSPIzMxEVlYWTpw4gaysLCxatAhHjx61/JuR5TRdQPNV3WsrBpb8g9+i6GIDvD0l2PB4AmSeHLdCRESmEQmCIJizQ3JyMqZPn478/HzDtpiYGCxcuBB5eXm92qempiItLQ1/+MMfDNuys7NRUlKCoqIiAEBmZibUajX2799vaDNv3jwEBgZi586dJtWlVqshl8uhUqng7+9vzleiH7pxGVgbD0ikwIo6QDz0K4enrqiwcNM/odEK+MNj0/DjJPvdUJGIiJyXqb/fZv0SdXZ2orS0FOnp6Ubb09PTUVxc3Oc+HR0dkMlkRtu8vb1x7NgxdHV1AdD1sPzwmHPnzu33mPrjqtVqowdZiWFKc4RVwkp7lwY5H5RDoxXw8NRQPJYYMeRjEhGRezHr16ihoQEajQYKhfECXwqFAnV1dX3uM3fuXLz33nsoLS2FIAgoKSnB1q1b0dXVhYaGBgBAXV2dWccEgLy8PMjlcsMjMpL/xW41Vp7S/F+FF1BZ34JgXyl+tzAOIhstREdERK7Lov98/uEPjiAI/f4Ivfrqq8jIyMCsWbPg6emJBQsW4KmnngIASCS3xjCYc0wAyM3NhUqlMjxqamos+SrUFysOuC25fB2bD18CoJvCHOTjNeRjEhGR+zErsAQHB0MikfTq+aivr+/VQ6Ln7e2NrVu3oq2tDZcvX0Z1dTXGjh0LPz8/BAcHAwBCQkLMOiYASKVS+Pv7Gz3ISqwUWNo6u/HrD09AEIAfTY/g0vtERGQxswKLl5cXEhMTUVhYaLS9sLAQqampA+7r6emJiIgISCQS7Nq1C/Pnz4e4Z3xESkpKr2MeOHBg0GOSjaisc0lo9f5zuNzYhlC5DK9xCjMREQ2Bh7k75OTkICsrC0lJSUhJScHmzZtRXV2NpUuXAtBdqqmtrTWstXLhwgUcO3YMycnJuHHjBtasWYPTp0/j/fffNxxz+fLlmDNnDlavXo0FCxbg008/xRdffGGYRUR21vSd7nkIgeWfFxvw/hHdcVb/aBrk3p7WqIyIiNyU2YElMzMTjY2NWLVqFZRKJeLi4rBv3z5ERUUBAJRKpdGaLBqNBn/6059w/vx5eHp64t5770VxcTHGjh1raJOamopdu3bhlVdewauvvorx48ejoKAAycnJQ/+GZB5NN6Cq1b22MLCo27vw7x+dBAA8MWsM5kwcZa3qiIjITZm9Douz4josVtJUDbwzFRB7Aq/UWzSt+d8/OoEPSq5gTNAI7F8+Gz5Ss3MxERG5CZusw0JuQD+l2cI1WP5+9ho+KLkCkQj444/jGVaIiMgqGFjI2BBmCN1o7cRvPj4FAFicFo2Z0UHWrIyIiNwYAwsZMwQW8xfie31vBb5v7sD4UT749dxJVi6MiIjcGQMLGVPpA0uUWbv930kl9p64ColYhD8tupM3NiQiIqviAAMydtslIY1WwPvFl7Huy0o0tXWZtPvz94zHnZEBtquPiIjcEgMLGesJLDXaYCz7czHKqptM3jU+MgAv3jfBRoUREZE7Y2ChW7QaCKpaiAD87KOrqNaMhK/UA7/JmIy5U0IG3X2kjxfEYt7YkIiIrI+BhQxOnz+POG0XugQJajUBuH/yaLz5r3EIlXs7ujQiInJzDCyEts5u/OnABZws3o8PvYBropH4r58m4ZFpoQPeMZuIiMheGFjcXFFlA3L3nETN9ZtYKG4AACgiJ+Bf4sMcXBkREdEtDCxu7NPyWizfVQ4ACJPLsGyiFDgFeI4c69C6iIiIfojrsLixv/7zMgBg4Z1hOJBzN8Z5NureGMJdmomIiGyBgcVNKVU3UV7TBJEI+O1DMfCVegxpWX4iIiJbYmBxU5+drgMAJI4JxGh/mW6j4caH5i/LT0REZEsMLG5q/yldYJkX17O+ilYLqHoCC3tYiIjIyTCwuKH65nYc/+46gNsCS8s1QNMJiCSAf7gDqyMiIuqNgcUNHai4BkEApkXIERE4QrdRP37FPwyQcPIYERE5FwYWN6Qfv2LoXQF4OYiIiJwaA4ubudHaiSOXdNOXM+JCb73R9J3umYGFiIicEAOLmyk8ew0arYDJIX6IDva59QanNBMRkRNjYHEz+stBRr0rAKc0ExGRU2NgcSPN7V0oqtTdLyhjaojxm+xhISIiJ8bA4ka+PFePTo0W40b5YMJo31tvCAIH3RIRkVNjYHEj+sXiMuJCIBKJbr3R+j3Q3Q5AxDVYiIjIKTGwuIm2zm58daEeQF/jV25bg8XDy86VERERDY6BxU0cPP892ru0iAj0xpQwf+M3OaWZiIicHAOLm9h/up/LQcCtHhbOECIiIifFwOIGOro1+PKc7nLQvB9eDgJuTWlmDwsRETkpBhY3UFTZgJaObij8pUiIDOjdgFOaiYjIyTGwuAH95aB5U0IgFot6N2BgISIiJ2dRYNm0aROio6Mhk8mQmJiIw4cPD9h+x44diI+Px4gRIxAaGoqnn34ajY2Nhve3bdsGkUjU69He3m5JeXSbLo0WhWeuAQAypvZxOYhrsBAR0TBgdmApKChAdnY2VqxYgbKyMsyePRsZGRmorq7us31RURGefPJJLF68GBUVFfjwww9x/PhxLFmyxKidv78/lEql0UMmk1n2rcjg60uNUN3sQrCvF2aMDerdoK0R6GrTvZZH2Lc4IiIiE5kdWNasWYPFixdjyZIliImJwTvvvIPIyEjk5+f32f7rr7/G2LFjsWzZMkRHR+Ouu+7CL37xC5SUlBi1E4lECAkJMXrQ0OkvBz0YGwJJn5eDeqY0+4UCHlI7VkZERGQ6swJLZ2cnSktLkZ6ebrQ9PT0dxcXFfe6TmpqKK1euYN++fRAEAdeuXcNHH32Ehx9+2KhdS0sLoqKiEBERgfnz56OsrGzAWjo6OqBWq40eZEyjFXCg4tZ05j7xpodERDQMmBVYGhoaoNFooFAojLYrFArU1dX1uU9qaip27NiBzMxMeHl5ISQkBAEBAVi/fr2hzeTJk7Ft2zbs3bsXO3fuhEwmQ1paGiorK/utJS8vD3K53PCIjOQP7g+VXL6OhpZOyL09kTJ+ZN+NOOCWiIiGAYsG3f5w4TFBEHovRtbjzJkzWLZsGV577TWUlpbis88+Q1VVFZYuXWpoM2vWLDzxxBOIj4/H7Nmz8cEHH2DixIlGoeaHcnNzoVKpDI+amhpLvopL018OeiBGAU9JP/9TM7AQEdEw4GFO4+DgYEgkkl69KfX19b16XfTy8vKQlpaGl19+GQAwbdo0+Pj4YPbs2XjzzTcRGtp75opYLMaMGTMG7GGRSqWQSjnmoj9arYDPB7scBDCwEBHRsGBWD4uXlxcSExNRWFhotL2wsBCpqal97tPW1gax2PhjJBIJAF3PTF8EQUB5eXmfYYZMc+JKE5Sqdvh4SXDXhOD+GxqmNPOSGhEROS+zelgAICcnB1lZWUhKSkJKSgo2b96M6upqwyWe3Nxc1NbWYvv27QCARx55BM8++yzy8/Mxd+5cKJVKZGdnY+bMmQgLCwMArFy5ErNmzcKECROgVquxbt06lJeXY+PGjVb8qu7ls57LQffFKCDzlPTdSBBu62GJslNlRERE5jM7sGRmZqKxsRGrVq2CUqlEXFwc9u3bh6go3Q+eUqk0WpPlqaeeQnNzMzZs2ICXXnoJAQEBuO+++7B69WpDm6amJjz33HOoq6uDXC5HQkICDh06hJkzZ1rhK7qnA/rF4ga6HHTzBtDZonvNNViIiMiJiYT+rssMM2q1GnK5HCqVCv7+/o4ux6Eufd+C+/50EJ4SEcpeS4evtJ9cerUc2Hw34DMaeLn/8UJERES2YurvN+8l5IL0d2ZOjh7Zf1gBOOCWiIiGDQYWF/SP87rAcu/k0QM3ZGAhIqJhgoHFxTS3d+FY1XUAwH2DBZar3+ieAzngloiInBsDi4spqmxAl0bAuGAfRAf79N+w+RpwZq/udewC+xRHRERkIQYWF6MfvzLo5aDSbYC2C4iYAYQl2L4wIiKiIWBgcSFarWAYv3L/QIGluxMo2ap7PfMXdqiMiIhoaBhYXMipWhUaWjrhK/VA0tig/hue+xvQUgf4Kng5iIiIhgUGFheivxw0e0IwvDwG+J/26Gbdc+LTgIeXHSojIiIaGgYWF6IPLAPODlKeAGq+BsQeQOJT9imMiIhoiBhYXES9uh2nalUAgHsmDRBYjvX0rsQuAPx5c0kiIhoeGFhchH6wbXxkAEb5Sftu1HYdOPWR7jUH2xIR0TDCwOIiDJeDBupd+eZ9oLsdCJkGRPLGkkRENHwwsLiAjm4NiiobAAwwfkXTDRzfonud/AtAJLJTdUREREPHwOICjlVdR2unBqP9pJgS1s+dLi98BqhqAO8gIO5H9i2QiIhoiBhYXIBhddtJoyEW99Nzcuxd3XPizwFPbztVRkREZB0MLMOcIAiDL8dffxaoOgSIxEDSM3asjoiIyDoYWIa5Sw2t+K6xDV4SMe6aENx3o2N/0T1PeggIGGO/4oiIiKyEgWWY+/KsrncleVwQfKUevRu0q4ATu3SvkzmVmYiIhicGlmFu0NVty3YAXa3AqBhg7Gw7VkZERGQ9DCzDmLq9C8cvXwfQT2DRaoHjPZeDZj7LqcxERDRsMbAMY4cvNKBbK2DcKB9EjfTp3eDbvwPXLwFSOTAt0/4FEhERWQkDyzCmvxx0f3+Xg472TGVOeAKQ+tqpKiIiIutjYBmmtFoBX50fYDpz47fAxUIAImDGYvsWR0REZGV9TCshp1f4OjpP78UHXTchloowdl8fl4PadXduxoQHgZHj7VsfERGRlTGwDDdt14F/vgMZgPH6/rHGAdqn/NIORREREdkWA8twU3MMAHBVHIrlN5/FC/fegXsmjeq77YhgYNREOxZHRERkGwwsw031EQDAoc5JKMFkxKU+APhKHVwUERGRbXHQ7XBT/TUAoESYhPiIAAQzrBARkRtgYBlOutqBq98AAI5rJ/U/nZmIiMjFMLAMJ8pyQNOJBkGO7wRF/3dnJiIicjEWBZZNmzYhOjoaMpkMiYmJOHz48IDtd+zYgfj4eIwYMQKhoaF4+umn0dhoPLVl9+7diI2NhVQqRWxsLPbs2WNJaa6tZ/zKce1E+Hh5IDbU38EFERER2YfZgaWgoADZ2dlYsWIFysrKMHv2bGRkZKC6urrP9kVFRXjyySexePFiVFRU4MMPP8Tx48exZMkSQ5sjR44gMzMTWVlZOHHiBLKysrBo0SIcPXrU8m/miqp156NEOxFx4XKIxbw3EBERuQeRIAiCOTskJydj+vTpyM/PN2yLiYnBwoULkZeX16v9H//4R+Tn5+Pbb781bFu/fj3efvtt1NTUAAAyMzOhVquxf/9+Q5t58+YhMDAQO3fuNKkutVoNuVwOlUoFf38X7HnQaoE/jANu3sCCjlVInp2O3z4U4+iqiIiIhsTU32+zelg6OztRWlqK9PR0o+3p6ekoLi7uc5/U1FRcuXIF+/btgyAIuHbtGj766CM8/PDDhjZHjhzpdcy5c+f2e0y31FgJ3LyBDkhRIYzF1HC5oysiIiKyG7MCS0NDAzQaDRQKhdF2hUKBurq6PvdJTU3Fjh07kJmZCS8vL4SEhCAgIADr1683tKmrqzPrmADQ0dEBtVpt9HBpPeNXyrTj0Q0PxEcEOLYeIiIiO7Jo0K1IZDx2QhCEXtv0zpw5g2XLluG1115DaWkpPvvsM1RVVWHp0qUWHxMA8vLyIJfLDY/IyEhLvsrw0bP+yjHtRASM8ERkkLeDCyIiIrIfswJLcHAwJBJJr56P+vr6Xj0kenl5eUhLS8PLL7+MadOmYe7cudi0aRO2bt0KpVIJAAgJCTHrmACQm5sLlUpleOjHw7isnsBSqp2EqeHyAcMcERGRqzErsHh5eSExMRGFhYVG2wsLC5GamtrnPm1tbRCLjT9GIpEA0PWiAEBKSkqvYx44cKDfYwKAVCqFv7+/0cNlNV8DblRBCxG+0U7g5SAiInI7Zt9LKCcnB1lZWUhKSkJKSgo2b96M6upqwyWe3Nxc1NbWYvv27QCARx55BM8++yzy8/Mxd+5cKJVKZGdnY+bMmQgLCwMALF++HHPmzMHq1auxYMECfPrpp/jiiy9QVFRkxa86jNXoeleqxGPRjBGYGsEBt0RE5F7MDiyZmZlobGzEqlWroFQqERcXh3379iEqKgoAoFQqjdZkeeqpp9Dc3IwNGzbgpZdeQkBAAO677z6sXr3a0CY1NRW7du3CK6+8gldffRXjx49HQUEBkpOTrfAVXUDP5aDirjsAgD0sRETkdsxeh8VZufQ6LJvvAa6WYVnnL3HE514c++39HMNCREQuwSbrsJADdLYCypMAdCvcxkdwwC0REbkfBhZnd6UEEDS44TEaVxGMabwcREREboiBxdn1jF/5BpMBgANuiYjILTGwOLueGUJf3RwHAJjGJfmJiMgNMbA4M60GqDkOACjRTkJ4gDdG+kodXBQREZH9MbA4s2sVQGczOiS+OC9EIj6SvStEROSeGFicWc/4lW+lMdBCjKnhAY6th4iIyEEYWJxZzx2a/9mpXzCOPSxEROSeGFiclSAYeli+bNMNuI1jYCEiIjfFwOKsVDVA81VoRR4o147HuGAf+Ms8HV0VERGRQzCwOKvqowCAep+JuAkZprF3hYiI3BgDi7PqGb9yUhwDAJjKFW6JiMiNMbA4q57xK39vjQbAAbdEROTeGFic0c0moP4MAODL1nEQi4ApYQwsRETkvhhYnNGV4wAEtPpG4XsEYKLCD95eEkdXRURE5DAMLM6oZ/xKlXccAHDALRERuT0GFmfUM0PoqGYiAA64JSIiYmBxNt2dQG0JAOB/b4wBwAG3REREDCzOpu4k0N0OjSwQZTdHw1MiwqQQP0dXRURE5FAMLM6mZ/zK94EJAESICfWH1IMDbomIyL0xsDibnvVXznjoFozjgFsiIiLAw9EFuA1BAC4XATevD9yup4flq5vjAQDTwgNsXBgREZHzY2Cxl/P7gF2Pm9RUkEjxt+8VAIBpkexhISIiYmCxl/P7dM8BUYB/+IBNG6Iexo1CEWSeYtwxytcOxRERETk3BhZ7EATg0iHd64fXABMeGLD54W+uADiBuDA5PCQcZkRERMRfQ3u4UQWoqgGxJxCVMmjzk1dUAIBpXDCOiIgIAAOLfVw6qHuOmAF4+Qza/OSVJgCcIURERKTHwGIPVT2BZdzdgzbt0mhRcVUNgIGFiIhIj4HF1rTaWz0s0YMHlsprLejo1sJP6oGxIwfvjSEiInIHDCy2du20bu0VTx8gPHHQ5vrLQVMj5BCLRTYujoiIaHhgYLE1/eWgqFTAw2vQ5idrdQNup/JyEBERkYFFgWXTpk2Ijo6GTCZDYmIiDh8+3G/bp556CiKRqNdjypQphjbbtm3rs017e7sl5TmXS6aPXwFu9bDEc4YQERGRgdmBpaCgANnZ2VixYgXKysowe/ZsZGRkoLq6us/2a9euhVKpNDxqamoQFBSEH//4x0bt/P39jdoplUrIZDLLvpWz6O4EvivWvTZh/Ep7lwbnlM0AOOCWiIjodmYHljVr1mDx4sVYsmQJYmJi8M477yAyMhL5+fl9tpfL5QgJCTE8SkpKcOPGDTz99NNG7UQikVG7kJAQy76RM6ktBbpagREjAUXcoM3P1zWjWysgyMcL4QHediiQiIhoeDArsHR2dqK0tBTp6elG29PT01FcXGzSMbZs2YIHHngAUVFRRttbWloQFRWFiIgIzJ8/H2VlZQMep6OjA2q12ujhdPTjV8bOBsSDn2r9dOYpYf4QiTjgloiISM+swNLQ0ACNRgOFQmG0XaFQoK6ubtD9lUol9u/fjyVLlhhtnzx5MrZt24a9e/di586dkMlkSEtLQ2VlZb/HysvLg1wuNzwiIyPN+Sr2Yeb4lYqrugG3sWH+tqqIiIhoWLJo0O0P/+tfEASTegS2bduGgIAALFy40Gj7rFmz8MQTTyA+Ph6zZ8/GBx98gIkTJ2L9+vX9His3NxcqlcrwqKmpseSr2E5nK3DluO61CeNXAOCMUt/DwvErREREtzPr5ofBwcGQSCS9elPq6+t79br8kCAI2Lp1K7KysuDlNfD0XrFYjBkzZgzYwyKVSiGVSk0v3t6+OwJouwD5GCBo3KDNNVrBMOB2CntYiIiIjJjVw+Ll5YXExEQUFhYabS8sLERqauqA+x48eBAXL17E4sWLB/0cQRBQXl6O0NBQc8pzLlVf6Z7HzQFM6H2qamjBzS4NvD0lXOGWiIjoB8zqYQGAnJwcZGVlISkpCSkpKdi8eTOqq6uxdOlSALpLNbW1tdi+fbvRflu2bEFycjLi4nrPllm5ciVmzZqFCRMmQK1WY926dSgvL8fGjRst/FpOwLAc/z0mNdcPuI0J9YOEK9wSEREZMTuwZGZmorGxEatWrYJSqURcXBz27dtnmPWjVCp7rcmiUqmwe/durF27ts9jNjU14bnnnkNdXR3kcjkSEhJw6NAhzJw504Kv5ATargN1p3Svo+eYtMuZqxy/QkRE1B+RIAiCo4uwBrVaDblcDpVKBX9/B48BqfgE+PDnwKgY4IWvTdrlifeOouhiA956dCp+MnOMbesjIiJyEqb+fvNeQrZQZd50ZkEQOKWZiIhoAAwstnDpK92zidOZlap23GjrgkQswkSFn+3qIiIiGqYYWKytqQa4fgkQiYGxaSbtoh+/MmG0L2SeEltWR0RENCwxsFib/nJQ2HRAZtoAWv0MIV4OIiIi6hsDi7WZuRw/cNuS/KEMLERERH1hYLEmQbjVw2Li+BXg9pseckozERFRXxhYrOn780DLNcBDBkQmm7SLqq0LtU03AfCSEBERUX8YWKxJ37sSmQx4ykzapUKpuxwUGeQNubenrSojIiIa1hhYrMmC8Sv6GUIcv0JERNQ/BhZr0XQDl4t0r6PvMXk3LslPREQ0OAYWa1GeADpUgFQOhN1p8m63Btyyh4WIiKg/DCzWUvWV7nnsXYDYtMXf2rs0uPh9CwD2sBAREQ2EgcVaLBi/cr6uGRqtgCAfLyj8pTYqjIiIaPhjYLGGrnag5qju9bh7TN7tjPLW5SCRSGSDwoiIiFwDA4s11BwFutsB3xAgeKLJu/EOzURERKZhYLGGy4d1z+PuBszoKeEKt0RERKZhYLGGhkrdc2i8ybtotALOKZsBcA0WIiKiwTCwWENTte45IMrkXaoaWnGzSwNvTwmig31sVBgREZFrYGCxhqbvdM+BpgcW/fiVmFA/SMQccEtERDQQBpah6mgB2hp1r+WRJu/GFW6JiIhMx8AyVKoa3bNMDngHmLybfsAtZwgRERENjoFlqG70XA4yY/yKIAhGa7AQERHRwBhYhsow4HaMybvUqdtxvbUTErEIExV+NiqMiIjIdTCwDJVhwO1Yk3epqNX1rkwY7QuZp2n3HSIiInJnDCxDpQ8sZvSwGMavcP0VIiIikzCwDJUFl4TOKLkkPxERkTkYWIbKgkG3XJKfiIjIPAwsQ9GuAtqbdK8DTFuDRdXWhSs3bgJgDwsREZGpGFiGoqlnDRbvIEBq2myfip7LQRGB3pB7e9qqMiIiIpfCwDIUFizJf2uFW/auEBERmcqiwLJp0yZER0dDJpMhMTERhw8f7rftU089BZFI1OsxZcoUo3a7d+9GbGwspFIpYmNjsWfPHktKsy9LBtxy/AoREZHZzA4sBQUFyM7OxooVK1BWVobZs2cjIyMD1dXVfbZfu3YtlEql4VFTU4OgoCD8+Mc/NrQ5cuQIMjMzkZWVhRMnTiArKwuLFi3C0aNHLf9m9mBBYKlgDwsREZHZRIIgCObskJycjOnTpyM/P9+wLSYmBgsXLkReXt6g+3/yySd49NFHUVVVhago3aWUzMxMqNVq7N+/39Bu3rx5CAwMxM6dO02qS61WQy6XQ6VSwd/fTmFg5+PA+f8DHvojMPPZQZu3d2kw5fXPodEKOJJ7H0Ll3nYokoiIyHmZ+vttVg9LZ2cnSktLkZ6ebrQ9PT0dxcXFJh1jy5YteOCBBwxhBdD1sPzwmHPnzh3wmB0dHVCr1UYPuzP0sJg2huXCtWZotAKCfLwQ4i+zYWFERESuxazA0tDQAI1GA4VCYbRdoVCgrq5u0P2VSiX279+PJUuWGG2vq6sz+5h5eXmQy+WGR2SkadOKrcrMS0K3Xw4SiUS2qoqIiMjlWDTo9oc/toIgmPQDvG3bNgQEBGDhwoVDPmZubi5UKpXhUVNTY1rx1nLzBtChm6JsemDhCrdERESW8DCncXBwMCQSSa+ej/r6+l49JD8kCAK2bt2KrKwseHl5Gb0XEhJi9jGlUimkUqk55VuXvnfFZxTgNcKkXXgPISIiIsuY1cPi5eWFxMREFBYWGm0vLCxEamrqgPsePHgQFy9exOLFi3u9l5KS0uuYBw4cGPSYDmXm5SCNVsA5ZTMATmkmIiIyl1k9LACQk5ODrKwsJCUlISUlBZs3b0Z1dTWWLl0KQHeppra2Ftu3bzfab8uWLUhOTkZcXFyvYy5fvhxz5szB6tWrsWDBAnz66af44osvUFRUZOHXsgMz7yFU1dCKm10aeHtKEB3sY8PCiIiIXI/ZgSUzMxONjY1YtWoVlEol4uLisG/fPsOsH6VS2WtNFpVKhd27d2Pt2rV9HjM1NRW7du3CK6+8gldffRXjx49HQUEBkpOTLfhKdmL2gFvd+JXJoX6QiDngloiIyBxmBxYAeP755/H888/3+d62bdt6bZPL5WhraxvwmI899hgee+wxS8pxDDMDS3lNEwAgPiLANvUQERG5MN5LyFJm3kdIH1gSxgTYph4iIiIXxsBiCUEwa9G4jm4NKmp1M4TujAywYWFERESuiYHFEjdvAJ0tutfywResO6tsRqdGiyAfL4wJMm0KNBEREd3CwGKJG5d1z74hgOfgS+yXVd8AoOtd4Qq3RERE5mNgsYSFA255OYiIiMgyDCyW0AcWEwfcllU3AeCAWyIiIksxsFhCP0PIhB6WxpYOVF/XTemexinNREREFmFgsYQZl4T0l4PuGO0LubenDYsiIiJyXQwsljBjSjPHrxAREQ0dA4u5jNZgGbyHheNXiIiIho6BxVytDUBXGwARII8YsKlWK+AEe1iIiIiGjIHFXPreFf8wwEM6YNNvv29Bc0c3vD0lmKTws0NxREREromBxVxNl3XPplwO6uldmRohh4eEp5qIiMhS/BU1F8evEBER2R0Di7lu6NdgMX2GUALHrxAREQ0JA4u5TOxhae3oxvk63R2aE8YE2roqIiIil8bAYi4TA8upWhW0AhAql0HhP/gNEomIiKh/DCzm0GpNvo+QfvwKpzMTERENHQOLOVrrAU0HIBID/uEDNi2vuQGAA26JiIisgYHFHIY1WMIBSf/3BRIE4bYeFo5fISIiGioGFnOYOENIqWpHfXMHJGIRpobL7VAYERGRa2NgMUeTPrAMPOBW37syOcQP3l4SGxdFRETk+hhYzGHiDCGOXyEiIrIuBhZz6HtYBpkhVG644SHHrxAREVkDA4s5TOhh6dJocfKKCgB7WIiIiKyFgcVUWi3QVKN7PUBgOV/XjI5uLfxlHoge6WOn4oiIiFwbA4upmpWAtgsQewB+Yf02K6vWjV+5c0wgxGKRvaojIiJyaQwspjJag8Wj32ZlhvErAbaviYiIyE0wsJjKxCX5y3umNPMOzURERNbDwGIqE9ZgaWrrxKWGVgDsYSEiIrImiwLLpk2bEB0dDZlMhsTERBw+fHjA9h0dHVixYgWioqIglUoxfvx4bN261fD+tm3bIBKJej3a29stKc82mgZf5VY/nXnsyBEI9PGyQ1FERETuof/BGP0oKChAdnY2Nm3ahLS0NLz77rvIyMjAmTNnMGZM370PixYtwrVr17BlyxbccccdqK+vR3d3t1Ebf39/nD9/3mibTCYztzzbMUxpHjywJIzh+itERETWZHZgWbNmDRYvXowlS5YAAN555x18/vnnyM/PR15eXq/2n332GQ4ePIhLly4hKCgIADB27Nhe7UQiEUJCQswtx35uDH5J6NYNDwNsXw8REZEbMeuSUGdnJ0pLS5Genm60PT09HcXFxX3us3fvXiQlJeHtt99GeHg4Jk6ciF//+te4efOmUbuWlhZERUUhIiIC8+fPR1lZmZlfxYY03YC6Vve6n8AiCMJtPSwB9qmLiIjITZjVw9LQ0ACNRgOFQmG0XaFQoK6urs99Ll26hKKiIshkMuzZswcNDQ14/vnncf36dcM4lsmTJ2Pbtm2YOnUq1Go11q5di7S0NJw4cQITJkzo87gdHR3o6Ogw/FmtVpvzVczTrAS03YDYE/AL7bNJVUMrVDe74OUhxuQQf9vVQkRE5IbMviQE6C7f3E4QhF7b9LRaLUQiEXbs2AG5XA5Ad1npsccew8aNG+Ht7Y1Zs2Zh1qxZhn3S0tIwffp0rF+/HuvWrevzuHl5eVi5cqUl5ZvPMOA2EhD33Sml712ZGi6HlwcnXxEREVmTWb+swcHBkEgkvXpT6uvre/W66IWGhiI8PNwQVgAgJiYGgiDgypUrfRclFmPGjBmorKzst5bc3FyoVCrDo6amxpyvYh4T7iHE8StERES2Y1Zg8fLyQmJiIgoLC422FxYWIjU1tc990tLScPXqVbS0tBi2XbhwAWKxGBEREX3uIwgCysvLERra9+UXAJBKpfD39zd62IxZM4QCbFcHERGRmzL72kVOTg7ee+89bN26FWfPnsWvfvUrVFdXY+nSpQB0PR9PPvmkof3jjz+OkSNH4umnn8aZM2dw6NAhvPzyy3jmmWfg7e0NAFi5ciU+//xzXLp0CeXl5Vi8eDHKy8sNx3S4QWYItXdpcFapG0PDHhYiIiLrM3sMS2ZmJhobG7Fq1SoolUrExcVh3759iIrS9T4olUpUV1cb2vv6+qKwsBAvvvgikpKSMHLkSCxatAhvvvmmoU1TUxOee+451NXVQS6XIyEhAYcOHcLMmTOt8BWtYJAeltO1KnRrBYzykyI8wNuOhREREbkHkSAIgqOLsAa1Wg25XA6VSmX9y0P/FQeoaoDFhUBk7xD1l0OX8Pt9Z/FgrAJ/eTLJup9NRETkwkz9/eZ0lsFougZdg+VkrQoALwcRERHZCgPLYNS1gKAFJFLAZ3SfTSp6AktcuLzP94mIiGhoGFgGc/uA2z7WYGlu7zLcoXlKGBeMIyIisgUGlsEMsgbLWWUzACBULkOwr9ReVREREbkVBpbBDBJYKq7qLgexd4WIiMh2GFgGo1+WP7C/Kc269VemhHH8ChERka0wsAzGxB4WDrglIiKyHYtufuhWwhMBkQQY2fuu0e1dGlTW6245EBfOS0JERES2wsAymLm/7/etc3XN0GgFjPTxQoi/zI5FERERuRdeEhoCw4DbcDlEIpGDqyEiInJdDCxDoB9wG8cZQkRERDbFwDIEt6Y0c8AtERGRLTGwWKhLo8W5nkXjOOCWiIjIthhYLFR5rQWdGi38ZB4YEzTC0eUQERG5NAYWC52+bYVbDrglIiKyLQYWC525qh9wy/ErREREtsbAYqHTtVzhloiIyF4YWCyg0Qo4o9TfQ4gDbomIiGyNgcUCVQ2taOvUQOYpxrhRvo4uh4iIyOUxsFhAv/5KbKg/JGIOuCUiIrI1BhYLcPwKERGRfTGwWKCCM4SIiIjsioHFTIIgGHpYpnCFWyIiIrtgYDHTlRs3oW7vhqdEhAmj/RxdDhERkVtgYDGTvndlUogfvDx4+oiIiOyBv7hm0i/Jz/ErRERE9sPAYib9gNspnCFERERkNwwsZrh9wG0cV7glIiKyGwYWM9Q3d6ChpRMSsQgxoQwsRERE9sLAYgZ978odo3wh85Q4uBoiIiL3wcBihtO1vOEhERGRI1gUWDZt2oTo6GjIZDIkJibi8OHDA7bv6OjAihUrEBUVBalUivHjx2Pr1q1GbXbv3o3Y2FhIpVLExsZiz549lpRmU/oZQhxwS0REZF9mB5aCggJkZ2djxYoVKCsrw+zZs5GRkYHq6up+91m0aBH+/ve/Y8uWLTh//jx27tyJyZMnG94/cuQIMjMzkZWVhRMnTiArKwuLFi3C0aNHLftWNnLGsCQ/e1iIiIjsSSQIgmDODsnJyZg+fTry8/MN22JiYrBw4ULk5eX1av/ZZ5/hJz/5CS5duoSgoKA+j5mZmQm1Wo39+/cbts2bNw+BgYHYuXOnSXWp1WrI5XKoVCr4+1s/UFxv7cT03xUCAE69kQ4/mafVP4OIiMjdmPr7bVYPS2dnJ0pLS5Genm60PT09HcXFxX3us3fvXiQlJeHtt99GeHg4Jk6ciF//+te4efOmoc2RI0d6HXPu3Ln9HhPQXWZSq9VGD1uq6LkcFB3sw7BCRERkZx7mNG5oaIBGo4FCoTDarlAoUFdX1+c+ly5dQlFREWQyGfbs2YOGhgY8//zzuH79umEcS11dnVnHBIC8vDysXLnSnPKHhANuiYiIHMeiQbcikcjoz4Ig9Nqmp9VqIRKJsGPHDsycORMPPfQQ1qxZg23bthn1sphzTADIzc2FSqUyPGpqaiz5KiYzDLjlkvxERER2Z1YPS3BwMCQSSa+ej/r6+l49JHqhoaEIDw+HXH7rhz4mJgaCIODKlSuYMGECQkJCzDomAEilUkilUnPKHxLDgNtw9rAQERHZm1k9LF5eXkhMTERhYaHR9sLCQqSmpva5T1paGq5evYqWlhbDtgsXLkAsFiMiIgIAkJKS0uuYBw4c6PeY9tbc3oWqhlYA7GEhIiJyBLMvCeXk5OC9997D1q1bcfbsWfzqV79CdXU1li5dCkB3qebJJ580tH/88ccxcuRIPP300zhz5gwOHTqEl19+Gc888wy8vb0BAMuXL8eBAwewevVqnDt3DqtXr8YXX3yB7Oxs63zLIdL3roQHeCPIx8vB1RAREbkfsy4JAbopyI2NjVi1ahWUSiXi4uKwb98+REVFAQCUSqXRmiy+vr4oLCzEiy++iKSkJIwcORKLFi3Cm2++aWiTmpqKXbt24ZVXXsGrr76K8ePHo6CgAMnJyVb4ikN3+ioH3BIRETmS2euwOCtbrsOSU1COj8tqkfPgRCy7f4JVj01EROTObLIOi7u6NUOIPSxERESOwMAyiJudGlys1w0YjuM9hIiIiByCgWUQ5+rU0ApAsK8Uo/3sN42aiIiIbmFgGcTp29ZfGWghOyIiIrIdBpZBVNTqxq/Ecf0VIiIih2FgGYR+wC1XuCUiInIcs9dhcTdPp0ajvKYJ8ZEBji6FiIjIbTGwDOJHiRH4UWKEo8sgIiJya7wkRERERE6PgYWIiIicHgMLEREROT0GFiIiInJ6DCxERETk9BhYiIiIyOkxsBAREZHTY2AhIiIip8fAQkRERE6PgYWIiIicHgMLEREROT0GFiIiInJ6DCxERETk9Fzmbs2CIAAA1Gq1gyshIiIiU+l/t/W/4/1xmcDS3NwMAIiMjHRwJURERGSu5uZmyOXyft8XCYNFmmFCq9Xi6tWr8PPzg0gkstpx1Wo1IiMjUVNTA39/f6sdl/rG821fPN/2xfNtXzzf9mXp+RYEAc3NzQgLC4NY3P9IFZfpYRGLxYiIiLDZ8f39/fkX3o54vu2L59u+eL7ti+fbviw53wP1rOhx0C0RERE5PQYWIiIicnoMLIOQSqV4/fXXIZVKHV2KW+D5ti+eb/vi+bYvnm/7svX5dplBt0REROS62MNCRERETo+BhYiIiJweAwsRERE5PQYWIiIicnoMLIPYtGkToqOjIZPJkJiYiMOHDzu6JJdw6NAhPPLIIwgLC4NIJMInn3xi9L4gCHjjjTcQFhYGb29v3HPPPaioqHBMscNcXl4eZsyYAT8/P4wePRoLFy7E+fPnjdrwfFtXfn4+pk2bZlhAKyUlBfv37ze8z/NtO3l5eRCJRMjOzjZs4/m2rjfeeAMikcjoERISYnjfVuebgWUABQUFyM7OxooVK1BWVobZs2cjIyMD1dXVji5t2GttbUV8fDw2bNjQ5/tvv/021qxZgw0bNuD48eMICQnBgw8+aLhnFJnu4MGDeOGFF/D111+jsLAQ3d3dSE9PR2trq6ENz7d1RURE4K233kJJSQlKSkpw3333YcGCBYZ/tHm+beP48ePYvHkzpk2bZrSd59v6pkyZAqVSaXicOnXK8J7NzrdA/Zo5c6awdOlSo22TJ08WfvOb3zioItcEQNizZ4/hz1qtVggJCRHeeustw7b29nZBLpcLf/7znx1QoWupr68XAAgHDx4UBIHn214CAwOF9957j+fbRpqbm4UJEyYIhYWFwt133y0sX75cEAT+/baF119/XYiPj+/zPVueb/aw9KOzsxOlpaVIT0832p6eno7i4mIHVeUeqqqqUFdXZ3TupVIp7r77bp57K1CpVACAoKAgADzftqbRaLBr1y60trYiJSWF59tGXnjhBTz88MN44IEHjLbzfNtGZWUlwsLCEB0djZ/85Ce4dOkSANueb5e5+aG1NTQ0QKPRQKFQGG1XKBSoq6tzUFXuQX9++zr33333nSNKchmCICAnJwd33XUX4uLiAPB828qpU6eQkpKC9vZ2+Pr6Ys+ePYiNjTX8o83zbT27du3CN998g+PHj/d6j3+/rS85ORnbt2/HxIkTce3aNbz55ptITU1FRUWFTc83A8sgRCKR0Z8FQei1jWyD5976fvnLX+LkyZMoKirq9R7Pt3VNmjQJ5eXlaGpqwu7du/Hzn/8cBw8eNLzP820dNTU1WL58OQ4cOACZTNZvO55v68nIyDC8njp1KlJSUjB+/Hi8//77mDVrFgDbnG9eEupHcHAwJBJJr96U+vr6XsmRrEs/2pzn3rpefPFF7N27F//4xz8QERFh2M7zbRteXl644447kJSUhLy8PMTHx2Pt2rU831ZWWlqK+vp6JCYmwsPDAx4eHjh48CDWrVsHDw8Pwznl+bYdHx8fTJ06FZWVlTb9+83A0g8vLy8kJiaisLDQaHthYSFSU1MdVJV7iI6ORkhIiNG57+zsxMGDB3nuLSAIAn75y1/i448/xpdffono6Gij93m+7UMQBHR0dPB8W9n999+PU6dOoby83PBISkrCz372M5SXl2PcuHE83zbW0dGBs2fPIjQ01LZ/v4c0ZNfF7dq1S/D09BS2bNkinDlzRsjOzhZ8fHyEy5cvO7q0Ya+5uVkoKysTysrKBADCmjVrhLKyMuG7774TBEEQ3nrrLUEulwsff/yxcOrUKeGnP/2pEBoaKqjVagdXPvz827/9myCXy4WvvvpKUCqVhkdbW5uhDc+3deXm5gqHDh0SqqqqhJMnTwq//e1vBbFYLBw4cEAQBJ5vW7t9lpAg8Hxb20svvSR89dVXwqVLl4Svv/5amD9/vuDn52f4bbTV+WZgGcTGjRuFqKgowcvLS5g+fbphKigNzT/+8Q8BQK/Hz3/+c0EQdFPjXn/9dSEkJESQSqXCnDlzhFOnTjm26GGqr/MMQPjrX/9qaMPzbV3PPPOM4d+NUaNGCffff78hrAgCz7et/TCw8HxbV2ZmphAaGip4enoKYWFhwqOPPipUVFQY3rfV+RYJgiAMrY+GiIiIyLY4hoWIiIicHgMLEREROT0GFiIiInJ6DCxERETk9BhYiIiIyOkxsBAREZHTY2AhIiIip8fAQkRERE6PgYWIiIicHgMLEREROT0GFiIiInJ6DCxERETk9P4/MAhiNRTeqccAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(output['train_metrics'])['f1_score'].plot()\n",
    "pd.DataFrame(output['valid_metrics'])['f1_score'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685ca51-871d-478b-bad7-cfbb9d1007ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd62204-f08a-4695-9d13-52279fd5346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedba1a-4d42-4a01-8283-358a8c992ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5549a-beae-4127-b7a3-20df34c784ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08115677-5609-4944-b3b7-756551e24850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba63f6b-1dfb-45c4-bbf0-8d0e2f807120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f6d5b-3b0d-489e-84b9-fe877175d027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938c254-318d-46a2-81e1-9ab085dbba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d123158-313d-416a-ba71-68dbf1302a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354da920-de52-4b12-9bd0-b463701ed6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d497aca-798a-4676-964b-eb949c8ff8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8822f-4d02-4d29-93b5-2f18207d8efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
